# Offline LTR via RL methods

## Overview

In this experiment, we mainly focus on the issue whether offline RL methods be efficient in LTR task. Unlike ***online methods*** mentioned by Guido et.al ([Reinforcement Online Learning to Rank with Unbiased Reward Shaping](https://arxiv.org/pdf/2201.01534.pdf)), which ***generates*** ranking lists by the ranking policy, then ***evaluates and updates*** the policy itself with click data simulated by the interaction between click model and ranking lists. However, these methods might lead to suboptimal results due to bias and noise of click data, same as the problems in online RL methods. As a result, we try ***offline RL methods*** in this experiment. 

## Folder Structure (Partially unfold)

```python
-- OFFLINE_CQL_LTR
​	-- experiment_output
​	-- SimpleSAC
​		-- PretrainRanker
​			-- Click_Model
​				-- AbstractClickModel.py
​				-- CM_model.py
​			-- dataset
​				-- MQ2007
​				-- MQ2008
​				-- AbstractDataset.py
​				-- Preprocess.py
​			-- Ranker
​				-- AbstractRanker.py
​				-- MDPRanker.py
​				-- MDPRankerV2.py
​				-- NeuralRanker.py
​			-- utils
​			-- run_MDP_cascade.py
​		-- __init__.py
​		-- model.py
​		-- offline_LTR.py
​		-- replay_buffer.py
​		-- sac.py
​		-- sampler.py
​		-- utils.py
​	-- viskit
```

## Algorithm

In the experiment, we use [Soft Actor Critic (SAC)](https://arxiv.org/pdf/1812.05905.pdf "Soft Actor Critic") as our training algorithm. ***The process to generate a ranking list under some query*** is modeled as an MDP, whose elements are defined as below:

- ***step***: each position in the ranking list.

- ***observation(state)***: the average feature of docs in previous and current positions.
- ***probability distribution(action)***: a probability distribution over all the unselected docs, generated by the policy.
- ***policy***: a mapping from observation to probability distribution, implemented by neural network.
- ***next observation***: the feature of a randomly sampled doc under the policy output .
- ***reward***: use biased <span style="color:red">click data</span> instead of unbiased relevance label, in each position i, use ***dcg@i*** matric.

## Process

### Generate Offline Click Dataset

Firstly, we need to create our ***offline click dataset***

```python
click_dataset = {
    qid1: {
        "clicked_doces":clicked_doces,
        "click_labels":np.array(click_labels),
        "propensities":obs_probs
    }
    qid1: {
        "clicked_doces":clicked_doces,
        "click_labels":np.array(click_labels),
        "propensities":obs_probs
    }
}
```

1. Pretrain Ranker with Online Methods, e.g. Online MDPRanker, and get logging policy.
2. Generate result list for each query with the logging policy, and simulate user clicks via our user model (Cascade assumption).
3. Form click dataset in the form as above.

### Train Offline

In the experiment, we totally train `n_epoch=2000` epochs. In each epoch:

1. ***rollout***: generate `n_env_steps_per_epoch=1000` samples and append them to `replay_buffer`.
2. ***training***: sample `n_train_step_per_epoch=1000` batches with `batch_size=256`. Send each batch into the training network and update policy.
3. ***evaluation***: happens every `eval_period=10` training. Sample `eval_n_trajs=5` trajectories and gets average return and average trajectory length.
4. ***record and plot***: record the metrics and plot curves via wandb.py.

Save the model after the whole process.

## Hyperparameters

```python
FLAGS_DEF = define_flags_with_default(
    '''offline training'''
    max_traj_length=10,  # max of the length is 10
    replay_buffer_size=1000000,
    seed=42,
    device='cuda',
    save_model=True,

    policy_arch='256-256',
    qf_arch='256-256',
    orthogonal_init=False,
    policy_log_std_multiplier=1.0,
    policy_log_std_offset=-1.0,

    n_epochs=2000,
    n_env_steps_per_epoch=1000,
    n_train_step_per_epoch=1000,
    eval_period=10,
    eval_n_trajs=5,

    '''online training'''
    batch_size=256,
    doc_feature_size = 46,  # MQ2007 dataset
    online_lr = 0.001,
    online_eta = 1,
    online_num_iteration=10,
    click_model = 'informational',  # "perfect", "informational", "navigational"
    reward_method = 'both', # "positive", "negative", "both"

    sac=SAC.get_default_config(),
    logging=WandBLogger.get_default_config(),
)
```

## Run Experiment

You can run SAC experiments using the following command:
```python
python -m SimpleSAC.offline_LTR --logging.output_dir ./experiment_output
```

## Visualize Experiments
You can visualize the experiment metrics with viskit:
```python
python -m viskit ./experiment_output
```
and simply navigate to [http://localhost:5000/](http://localhost:5000/)

## Weights and Biases Online Visualization Integration

See results at [results](https://wandb.ai/zeyuzhang/SimpleSAC--sac/runs/a5f144be0a47429b86e2b2318f1d8939?workspace=user-zeyuzhang) 

